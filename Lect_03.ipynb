{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Intro to Deep Learning (numpy only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "1. [Perceptron](#an)\n",
    "2. [Multi-Layer Perceptron (MLP)](#ann)\n",
    "3. [Convolutional Networks](#cnn)\n",
    "4. [Recurrent Networks](#rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a class=\"anchor\" id=\"an\">Perceptron</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, let $\\mathcal{D} = \\{(\\mathbf{x}_i \\in \\mathbb{R}^d, y_i \\in \\{0, 1\\})\\}_{i=1}^n$ be our training dataset. The samples are **i.i.d**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Perceptron** is a binary classification model, introduced by [F. Rosemblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) in the late 50's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The prediction function of the perceptron is \n",
    "$$\n",
    "\\mathbf{x} \\mapsto f(\\mathbf{x}; \\theta=\\{\\mathbf{w}, b\\}) = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\langle \\mathbf{x}, \\mathbf{w} \\rangle + b > 0, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure gives a representation of the Perceptron as an **Artificial Neuron**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/perceptron.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the **activation function** can be any *non-linear* function. Example of basic activation functions\n",
    "- the **threshold** function (used by the standard Perceptron):\n",
    "$$\n",
    "    x \\mapsto  \\begin{cases}\n",
    "        1 & \\text{if } x > 0 \\\\\n",
    "        0 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "- the **sigmoid** function (used by the Logistic Regression):\n",
    "$$\n",
    " x \\mapsto \\frac{1}{1 + \\exp(-x)},\n",
    "$$\n",
    "\n",
    "- the **tanh** (hyperbolic tangent) function:\n",
    "$$\n",
    " x \\mapsto \\frac{\\exp(2x) -1}{\\exp(2x) + 1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Perceptron learning rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Update Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's change our inputs and add a $1$ in the first dimension, that is $\\mathbf{x} = [1, x_1, \\cdots, x_d]$ and $\\mathbf{w} = [b, w_1, \\cdots, w_d]$.\n",
    "\n",
    "The Perceptron update rule of our parameter vector $\\mathbf{w}$, for a training sample $(\\mathbf{x}_i, y_i)$, is given by\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\mathbf{w} + \\eta  \\big(y_i -  f(\\mathbf{x}_i; \\theta) \\big)\\mathbf{x}_i = \n",
    "\\begin{cases}\n",
    "        \\mathbf{w}  & \\text{if } y_i =  f(\\mathbf{x}_i; \\theta) \\\\\n",
    "        \\mathbf{w} + \\eta \\mathbf{x}_i & \\text{if } y_i = 1 \\text{ and } f(\\mathbf{x}_i; \\theta) = 0\\\\\n",
    "        \\mathbf{w} - \\eta \\mathbf{x}_i & \\text{if } y_i = 0 \\text{ and } f(\\mathbf{x}_i; \\theta) = 1\\\\\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    ">Generaly, $\\eta = 1$ for the Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Interpretation of the Update Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Perceptron update rule, notice that\n",
    "- if $ y_i = 1 \\text{ and } f(\\mathbf{x}_i; \\theta) = 0$, that is $\\langle \\mathbf{x}, \\mathbf{w} \\rangle < 0$ *i.e.* the $\\cos(\\angle (\\mathbf{x}_i, \\mathbf{w} )) < 0$, therefore we need to decrease the angle. And, by using the rule $\\mathbf{w}_{new} = \\mathbf{w} + \\eta \\mathbf{x}_i$, we get\n",
    "\n",
    "$$\\cos(\\angle (\\mathbf{x}_i, \\mathbf{w}_{new} )) \\sim \\langle \\mathbf{x}, \\mathbf{w}_{new} \\rangle = \\langle \\mathbf{x}_i, \\mathbf{w} \\rangle + \\eta\\langle \\mathbf{x}_i, \\mathbf{x}_i \\rangle >  \\langle \\mathbf{x}_i, \\mathbf{w} \\rangle  \\sim \\cos(\\angle (\\mathbf{x}_i, \\mathbf{w} )),  $$\n",
    "\n",
    "- similarly if $ y_i = 0 \\text{ and } f(\\mathbf{x}_i; \\theta) = 1$,   then $\\cos(\\angle (\\mathbf{x}_i, \\mathbf{w} )) > 0$, therefore we need to increase the angle. And, by using the rule $\\mathbf{w}_{new} = \\mathbf{w} - \\eta \\mathbf{x}_i$, we get \n",
    "\n",
    "$$\\cos(\\angle (\\mathbf{x}_i, \\mathbf{w}_{new} ))  \\sim \\langle \\mathbf{x}, \\mathbf{w}_{new} \\rangle = \\langle \\mathbf{x}_i, \\mathbf{w} \\rangle - \\eta\\langle \\mathbf{x}_i, \\mathbf{x}_i \\rangle <  \\langle \\mathbf{x}_i, \\mathbf{w} \\rangle  \\sim \\cos(\\angle (\\mathbf{x}_i, \\mathbf{w} )).  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is defined as\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n \\big(y_i - f(\\mathbf{x}_i; \\theta)\\big)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Coding our Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Designing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MyPerceptron:\n",
    "    def __init__(self, n_iter, lr=1):\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "    \n",
    "        self.losses = []\n",
    "        self.grads = []\n",
    "        \n",
    "    def init_params(self, n_feats):\n",
    "        self.w = np.zeros((1, n_feats))\n",
    "        self.b = 0.\n",
    "    \n",
    "    def _optimize(self, X, y):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # cost function \n",
    "        preds = np.squeeze(self.activation(X))\n",
    "        distances = np.squeeze(preds != y)\n",
    "        cost = np.mean(distances)\n",
    "        \n",
    "        \"\"\"\n",
    "        # computing the rules (for loop version )\n",
    "        dw = 0; db = 0\n",
    "        for i in range(len(X)): \n",
    "            if distances[i]:\n",
    "                if preds[i] == 0 and y[i] ==1:\n",
    "                    dw += -(1/m)*X[i,:] \n",
    "                    db += -(1/m)\n",
    "                elif preds[i] == 1 and y[i] ==0:\n",
    "                    dw += (1/m)*X[i,:]\n",
    "                    db += (1/m)\n",
    "        \"\"\"\n",
    "        # computing the rules (vectorized version)\n",
    "        dw = (1/m)*np.dot(preds - y, X)\n",
    "        db = (1/m)*np.sum(preds - y)\n",
    "        \n",
    "        grads = {\"dLdw\": dw, \"dLdb\": db}\n",
    "\n",
    "        return grads, cost\n",
    "\n",
    "    def activation(self, X):  \n",
    "        \"\"\"\n",
    "            Compute the Sigmoid activation on the dataset\n",
    "            \n",
    "            X: (np-array) shape=(n_samples x n_feats)\n",
    "                data matrix \n",
    "        \"\"\"          \n",
    "        return np.dot(self.w, X.T) + self.b > 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "            Training the model\n",
    "            \n",
    "            X: (np-array) shape=(n_samples x n_feats)\n",
    "                data matrix\n",
    "            y: (np-array) shape=( n_samples)\n",
    "                targets \n",
    "        \"\"\"\n",
    "        \n",
    "        self.init_params(X.shape[1])\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            grads, cost = self._optimize(X, y)\n",
    "            #\n",
    "            dLdw = grads['dLdw']\n",
    "            dLdb = grads['dLdb']\n",
    "            \n",
    "            # gradient descent\n",
    "            self.w = self.w - self.lr * dLdw\n",
    "            self.b = self.b - self.lr * dLdb\n",
    "            \n",
    "            self.losses.append(cost)\n",
    "            self.grads.append(grads)\n",
    "            #if (i % 10 == 0):\n",
    "            #    print(\"Standard Perceptron: Iter {}, Cost {}\".format(i, cost))\n",
    "            \n",
    "        print(\"Standard Perceptron: Iter {}, Cost {}\".format(i, cost))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            Predicting the discrete labels\n",
    "            \n",
    "            X: (np-array) shape=(n_samples x n_feats)\n",
    "                data matrix \n",
    "        \"\"\"\n",
    "        return self.activation(X)\n",
    "        \n",
    "    \n",
    "    def score(self, X, y): \n",
    "        \"\"\"\n",
    "            computing the accuracy of the model\n",
    "            \n",
    "            X: (np-array) shape=(n_samples x n_feats)\n",
    "                data matrix \n",
    "        \"\"\"\n",
    "        pred = self.predict(X)\n",
    "        return (pred == y).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def print_decision(X, y, clf, title=\"Decision Boundary\"):\n",
    "    plt.figure()\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "    ax.contourf(xx, yy, Z, alpha=.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black', s=25) \n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create 200 separable points\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=0, n_clusters_per_class=2, class_sep=2.0)\n",
    "\n",
    "#plt.figure()\n",
    "#plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black', s=25) \n",
    "#plt.title('Plot of the generated data and their label')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyPerceptron(n_iter=100, lr=1.)\n",
    "my_clf.fit(X, y)\n",
    "print_decision(X, y,  my_clf, \"Decision Boundary of our Perceptron model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_points = np.random.randn(10, 2)\n",
    "print_decision(rand_points, np.ones(10), my_clf, \"Boundary on random samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a class=\"anchor\" id=\"ann\" >Feed-Forward models</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, let $\\mathcal{D} = \\{(\\mathbf{x}_i \\in \\mathbb{R}^d,  y \\in \\mathbb{T})\\}_{i=1}^n$ be our training dataset. The samples are **i.i.d**.\n",
    "- For classification tasks $\\mathbb{T} = \\{0, \\cdots, C-1\\}$, where $C$ is number of classes\n",
    "- And, for regression tasks $\\mathbb{T} = \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Basic idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen the Artificial Neuron (the Perceptron), and now we need to connect it to other neurons.\n",
    "> Just like in nature neurons are interconnected to each other to obtain a complex network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use a **feed-forward** representation to create our network:\n",
    "- the artificial neurons are packed in **layers**\n",
    "- each layer contains a certain number of neurons that do not interact with each other\n",
    "- layers are concatened hierarchically (a layer is connected to a previous and next one)\n",
    "- a neuron in a layer takes as input all the outputs of the previous layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation is known as the **multi-layer perceptron (MLP)** or simple the **artificial neural network (ANN)**.\n",
    "\n",
    "The figure below illustrates a basic MLP (with 2 layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ann0.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Formal definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feed-forward neural network $f(\\cdot; \\theta)$ is a **directed acyclic graph**, parametrized by $\\theta$, \n",
    "that \n",
    "- applies a series of transformation to an input $\\mathbf{x} \\in \\mathbb{R}^d$, layer-wise, \n",
    "- without recursion, \n",
    "\n",
    "to produce an output $\\mathbf{y} = f(\\mathbf{x}, \\theta) \\in \\mathbb{R}^C$ , as depicted in below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/layer_wise_fnn.png\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a $K$ layer network -- here the input is not considered as a layer --, the $k$-th layer is characterized by a function\n",
    "$f_k$ parametrized by $\\theta_k$. \n",
    "In other words, \n",
    "> $f(\\mathbf{x}, \\theta) = f_K(\\dots f_2(f_1(\\mathbf{x}; \\theta_1); \\theta_2) \\dots; \\theta_K), \\quad \\forall \\mathbf{x} \\in \\mathcal{I}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $f_k$'s are of the form \n",
    "> $\n",
    " f_k(\\mathbf{z}_{k-1}; \\theta_k) = \\mathbf{z}_k = \\sigma_k(\\mathbf{q}_{k}) = \\sigma_k(\\mathbf{W}_k\\mathbf{z}_{k-1} + \\mathbf{b}_k),\n",
    "$\n",
    "\n",
    "where \n",
    "- $\\theta_k = \\{\\mathbf{W}_k, \\mathbf{b}_k\\}$,\n",
    "- $\\mathbf{W}_k \\in \\mathbb{R}^{h_k\\times h_{k-1}}$ is known as the **weight matrix**, \n",
    "- $\\mathbf{b}_k \\in \\mathbb{R}^{h_k}$ is the **bias vector**, \n",
    "- $\\mathbf{z}_k \\in \\mathbb{R}^{h_k}$ is the output of the $k$-th layer, \n",
    "- $h_k$ is the dimension (the number of neurons) of the $k$-th layer, \n",
    "- $\\sigma_k$ is a point-wise operator known as **activation function** of the layer, \n",
    "- and $\\mathbf{q}_k$ is the **pre-activation vector**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below gives another illustration on the interaction between two layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ann1.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we have seen three activation functions. \n",
    "Here are additional examples used in ANN:  \n",
    "- **softmax**:  $\\mathbf{t} \\in \\mathbb{R}^C  \\mapsto [\\dots, \\frac{\\exp(\\mathbf{x}[i])}{\\sum_{j=1}^C \\exp(\\mathbf{x}[j])}, \\dots]$,\n",
    "- **ReLU**:  $x \\mapsto \\max(0,x)$ (The Rectified Linear Unit ), \n",
    "- **PReLU**:  $x \\mapsto \\max(0,x) + a \\min(0, x)$ (Parametric ReLU, with parameter $a$),\n",
    "- **ELU**:  $x \\mapsto \\max(0,x) + a \\min(0, \\exp(x) -1)$ (Exponential Linear Unit, with parameter $a$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that, \n",
    "- differentiables activations (sigmoid, softmax, tanh) are prefered on an output layer,\n",
    "- *linear unit* based are prefered on hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "sigmoid = lambda t : 1./(1. + np.exp(-t))\n",
    "tanh = lambda t : (np.exp(2*t) - 1.)/(np.exp(2*t) + 1. )\n",
    "relu = lambda t : np.maximum(0, t)\n",
    "prelu = lambda t, a : np.maximum(0, t) + a *np.minimum(0, t)\n",
    "elu = lambda t, a : np.maximum(0, t) + a * np.minimum(0, np.exp(t) -1)\n",
    "\n",
    "xs = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(xs, sigmoid(xs))\n",
    "plt.title('Sigmoid')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(xs, relu(xs))\n",
    "plt.title('ReLU')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(xs, prelu(xs, 0.1))\n",
    "plt.title('PReLU')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(xs, elu(xs, 0.1))\n",
    "plt.title('ELU')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustration of a complete ANN which \n",
    "- takes $9$-dimensional vectors as inputs,\n",
    "- has $2$ hidden layers: the first layer has $4$ neurons, and the second has $2$,\n",
    "- outputs a single real number.\n",
    "\n",
    ">This architecture can be used, on $9$-dimensional, for\n",
    "- classification into two class,\n",
    "- regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ann2.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various loss functions are used to train ANNs.\n",
    "Depending on the tasks, a certain may be prefered to others.\n",
    "\n",
    "> In this subsection, we will use an ANN of $K$ layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 MSE loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Euclidian loss can be used for almost any task (classification and regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Euclidian loss averages the **Euclidian distances** ($\\ell_2$-norms) between targets and predictions.  $\\mathcal{L}$ is \n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n \\|\\mathbf{y}_i - f(\\mathbf{x}_i; \\theta)\\|_2^2 =  \\frac{1}{2n} \\|\\mathbf{Y} - f(\\mathbf{X}; \\theta)\\|_F^2,\n",
    "$$\n",
    "where $\\mathbf{Y} = [\\mathbf{y}_1, \\cdots, \\mathbf{y}_n]$, $\\mathbf{X} = [\\mathbf{x}_1, \\cdots, \\mathbf{x}_n]$, and $f(\\mathbf{X}; \\theta) = [f(\\mathbf{x}_1; \\theta), \\cdots, f(\\mathbf{x}_n; \\theta)]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Cross-Entropy (Negative Log-likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy assumes that\n",
    "- the targets are binary $\\{0, 1\\}$,\n",
    "- the activation of the final layer is a Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete derivation of the negative log-likelihood is given in the *Binary Logistic Regression* section of the [previous lecture](Lect_02.ipynb#blr-loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The cross-entropy loss, $\\mathcal{L}$, is defined as \n",
    "$$\n",
    "\\mathcal{L}(\\theta) = - \\frac{1}{n}\\sum_{i=1}^n y_i \\log f(\\mathbf{x}_i; \\theta) + (1-y_i) \\log (1 - f(\\mathbf{x}_i; \\theta)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Softmax Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax loss assumes that\n",
    "- the targets belongs to $\\{0, C-1\\}$, where $C$ is the number of classes,\n",
    "- the activation of the final layer is a Softmax,\n",
    "- $f(\\mathbf{x}; \\theta) \\in \\mathbb{R}^C$, where $f(\\mathbf{x}; \\theta)[k]$ is the probability of belonging to the $k$-th class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete derivation of the Softmax loss (*a.k.a* the multinomial logistic regression loss) is given in the *Multiclass Logistic Regression* section of the [previous lecture](Lect_02.ipynb#blr-loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Softmax loss, $\\mathcal{L}$, is defined as \n",
    "$$ \\mathcal{L}(\\theta) = - \\sum_{k=1}^C {\\mathbb{1}\\{y=k\\}} \\log f(\\mathbf{x}; \\theta) [k].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is divided as follows\n",
    "- [2.2.1](#backprop-lastlayer-matrixform) gives the gradients of the loss w.r.t the last layers' parameters (with additional vectorized form proof),\n",
    "- [2.2.2](#backprop-nonmatrix) gives the complete derivation to obtain the derivative an element in the $k$-th layers' weight matrix (no vector form),\n",
    "- [2.2.3](#backprop-matrix) gives the complete derivation in a vectorized form for the entire weight matrix of the $k$-th layer,\n",
    "- [2.2.4](#backprop-algo) gives the actual backpropagation algorithm used to updates parameters of any layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before writing the derivatives, let's recall some notations:\n",
    "- $f_k(\\cdot; \\theta) = \\mathbf{z}_k$, the output of the $k$-th layer,\n",
    "- $\\mathbf{z}_k = \\sigma_k\\big(\\mathbf{W}_k\\mathbf{z}_{k-1} + \\mathbf{b}_k\\big)$,\n",
    "- $\\mathbf{q}_k = \\mathbf{W}_k\\mathbf{z}_{k-1} + \\mathbf{b}_k$, value of the $k$-th layer before activation,\n",
    "- $w_{i,j}^{(k)} = \\mathbf{W}_k[i,j]$, the weight in the $i$-th row and $j$-th column of the $k$-th layer's weight matrix $\\mathbf{W}_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 <a class=\"anchor\" id=\"backprop-lastlayer-matrixform\">Backprop: Derivations w.r.t. to the Last Layer</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The gradient of the  parameters of the last layer ( $\\mathbf{W}_{K}, \\mathbf{b}_K$) are as follows\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\nabla_{\\mathbf{W}_{K}} \\mathcal{L} (\\theta) \n",
    "        &= \\Big[\\big(\\nabla_{\\mathbf{z}_{K}} \\mathcal{L} (\\theta) \\big) \\odot \\sigma_K'(\\mathbf{q}_{K}) \\Big] \\big( \\mathbf{z}_{K-1}^T\\big),\\\\\n",
    "        &\\\\\n",
    "        \\nabla_{\\mathbf{b}_{K}} \\mathcal{L} (\\theta) &= \\big(\\nabla_{\\mathbf{z}_{K}} \\mathcal{L} (\\theta) \\big) \\odot \\sigma_K'(\\mathbf{q}_{K}) ,\\\\\n",
    "        &\\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where \n",
    "- $\\sigma_K'(\\mathbf{q}_{k})$ is a point-wise derivative on each element of the vector $\\mathbf{q}_{k}$, \n",
    "- $\\odot$ is a point-wise product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Proof**: Here, we will give the complete derivation to obtain the previous gradients in a matrix form (see [here](#backprop-nonmatrix) for the non-matrix based derivations)\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "    &\\\\\n",
    "        \\nabla_{\\mathbf{W}_{K}} \\mathcal{L} (\\theta) &= \\big(\\nabla_{\\mathbf{q}_{K}} \\mathcal{L} (\\theta) \\big) ^T \\big(\\nabla_{\\mathbf{W}_{K}} \\mathbf{q}_{K}\\big), & \\text{ \\ \\ using the chain rule }\\\\\n",
    "        &= \\big(\\nabla_{\\mathbf{z}_{K}} \\mathcal{L} (\\theta) \\big)^T \\left\\{ \\nabla_{\\mathbf{W}_{K}} \\mathbf{q}_{K}[i] \\right\\}_{i=1}^{h_k}, & \\text{ vector w.r.t matrix } \\\\\n",
    "        &= \\big(\\nabla_{\\mathbf{z}_{K}} \\mathcal{L} (\\theta) \\big)^T\n",
    "        \\left\\{ \\begin{bmatrix}\n",
    "\\frac{\\partial \\mathbf{q}_{K}[i]}{\\partial w_{1,1}^{(K)}} & \\frac{\\partial \\mathbf{q}_{K}[i]}{\\partial w_{1,2}^{(K)}}&\\cdots & \\frac{\\partial \\mathbf{q}_{K}[i]}{\\partial w_{1,h_{K-1}}^{(K)}}\\\\\n",
    "\\frac{\\partial \\mathbf{q}_{K}[i]}{\\partial w_{2,1}^{(K)}} &\\frac{\\partial \\mathbf{q}_{K}[i]}{\\partial w_{2,2}^{(K)}}&\\cdots & \\frac{\\partial \\mathbf{q}_{K}[i]}{\\partial w_{2,h_{K-1}}^{(K)}}\\\\\n",
    "\\vdots & \\ddots & &\\vdots\\\\ \n",
    "\\frac{\\partial \\mathbf{q}_{K}[i]}{\\partial w_{h_K,1}^{(K)}}& &\\cdots& \\frac{\\partial \\mathbf{q}_{K}[i]}{\\partial w_{h_K,h_{K-1}}^{(K)}}\n",
    "\\end{bmatrix} \\right\\}_{i=1}^{h_k}, & \\text{ Jacobian matrix }\\\\ \n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{q}_{K}[i]}{\\partial  w_{j,l}^{(K)}} = \n",
    "\\begin{cases}\n",
    "0 & \\text{ if } i \\ne j \\\\\n",
    "\\mathbf{z}_{K-1}[l] & \\text{ otherwise }\n",
    "\\end{cases} \\quad \\text{since } \\mathbf{q}_{k}[p] = \\sum_{c=1}^{h_k} w_{p,c}^{(k)} \\mathbf{z}_{k-1}[c] + \\mathbf{b}_{k}[p]\n",
    "$$\n",
    "Therfore,\n",
    "$$\n",
    "\\nabla_{\\mathbf{W}_{K}} \\mathbf{q}_{K}[i] = \n",
    "\\begin{bmatrix} \\mathbf{z}_{K-1}^T \\\\\n",
    "\\mathbf{0} \\\\\n",
    "\\end{bmatrix}, \\quad  \\text{ with } \\mathbf{0} \\in \\{0\\}^{(h_K-1) \\times h_{K-1}}.$$\n",
    "Let's assume that\n",
    "$$\\nabla_{\\mathbf{z}_{K}} \\mathcal{L} (\\theta) = [a_1, a_2, \\cdots, a_K]^T.$$\n",
    "Then, we can write\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "\\big(\\nabla_{\\mathbf{z}_{K}} \\mathcal{L} (\\theta) \\big)^T \\left\\{ \\nabla_{\\mathbf{W}_{K}} \\mathbf{q}_{K}[i] \\right\\}_{i=1}^{h_k} &= \\{a_i \\mathbf{z}_{K-1}^T\\}_{i=1}^{h_k} \\\\\n",
    "&= [a_1, a_2, \\cdots, a_K]^T \\mathbf{z}_{K-1}^T =  \\big(\\nabla_{\\mathbf{z}_{K}} \\mathcal{L} (\\theta) \\big) \\mathbf{z}_{K-1}^T. \\\\\n",
    "&\\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$\n",
    "So the current expression of the gradient is\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "    &\\\\\n",
    "        \\nabla_{\\mathbf{W}_{K}} \\mathcal{L} (\\theta) &= \\big(\\nabla_{\\mathbf{q}_{K}} \\mathcal{L} (\\theta) \\big) \\big(\\mathbf{z}_{K-1}^T\\big), \\\\\n",
    "        &= \\left[\\big(\\nabla_{\\mathbf{z}_{K}} \\mathcal{L} (\\theta) \\big) \\big(\\nabla_{\\mathbf{q}_{K}} \\mathbf{z}_{K}\\big) \\right] \\big(\\mathbf{z}_{K-1}^T\\big),\\\\ \n",
    "        &\\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$\n",
    "Let's analyze the expression of the Jacobian matrix $\\nabla_{\\mathbf{q}_{k}} \\mathbf{z}_{k}$, for some $k$-th layer.\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "    &\\\\\n",
    "\\nabla_{\\mathbf{q}_{k}} \\mathbf{z}_{k} &= \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\mathbf{z}_{k}[1]}{\\partial \\mathbf{q}_{k}[1]} & \\frac{\\partial \\mathbf{z}_{k}[1]}{\\partial \\mathbf{q}_{k}[2]}&\\cdots & \\frac{\\partial \\mathbf{z}_{k}[1]}{\\partial \\mathbf{q}_{k-1}[h_{k}]}\\\\\n",
    "\\frac{\\partial \\mathbf{z}_{k}[2]}{\\partial \\mathbf{q}_{k}[2]} & \\frac{\\partial \\mathbf{z}_{k}[2]}{\\partial \\mathbf{q}_{k}[2]}&\\cdots & \\frac{\\partial \\mathbf{z}_{k}[2]}{\\partial \\mathbf{q}_{k}[h_{k}]}\\\\\n",
    "\\vdots & \\ddots & &\\vdots\\\\ \n",
    "\\frac{\\partial \\mathbf{z}_{k}[h_k]}{\\partial \\mathbf{q}_{k}[1]} & &\\cdots& \\frac{\\partial \\mathbf{z}_{k}[h_k]}{\\partial \\mathbf{q}_{k}[h_{k}]}\n",
    "\\end{bmatrix},\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\frac{\\partial \\mathbf{z}_{k}[i]}{\\partial \\mathbf{q}_{k}[j]} = \\frac{\\partial \\sigma_{k}(\\mathbf{q}_{k}[i])}{\\partial \\mathbf{q}_{k}[j]} = 0 \\quad \\forall i \\ne j.\\\\\\\\\n",
    "$$\n",
    "Therefore, $\\nabla_{\\mathbf{q}_{k}} \\mathbf{z}_{k}$ is a diagonal matrix, and we will use the notation \n",
    "$$\n",
    "\\big(\\nabla_{\\mathbf{z}_{k}} \\mathcal{L} (\\theta)\\big)^T  \\big(\\nabla_{\\mathbf{q}_{k}} \\mathbf{z}_{k} \\big) = \\big(\\nabla_{\\mathbf{z}_{k}} \\mathcal{L} (\\theta)\\big) \\odot \\sigma_k'(\\mathbf{q}_{k}).\n",
    "$$\n",
    "This conclude our proof (the derivation w.r.t the bias vector follows the same structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previously computed gradients, we can update all the **weights of the output layer** using standard gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Recalling the gradient descent rule, for a parameter set $\\theta$.** Let $\\theta^t$ be the value of $\\theta$ at the $t$-th iteration, $\\theta^0$ being the initialization of the parameters of the network,\n",
    "the update rule of the standard gradient descent  is as follows,\n",
    "$$\n",
    "\\theta^{t+1} \\leftarrow \\theta^{t} - \\eta \\nabla_{\\theta^{t}}\\mathcal{L}(\\theta^{t}) , \n",
    "$$\n",
    "where $\\nabla_{\\theta} \\mathcal{L}(\\theta)$ is the **gradient** of $\\mathcal{L}$ w.r.t the $\\theta$, and $\\eta$ is the step-size also known as **learning rate**. The parameters in $\\theta$ are updated sequentially layer after another,\n",
    "starting with the output one, by back-propagating the loss value, as illustrated in the figure below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: How about the parameters in the hidden layers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: Using the following two steps.\n",
    "1. We use the **chain rule** to compute the gradients of the loss w.r.t the hidden parameters,\n",
    "2. Then, the gradients are **backpropagated** in the network to update the hidden parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below illustrates the backpropagation procedure, for a certain loss function $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/layer_wise_fnn_bprop.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 <a class=\"anchor\" id=\"backprop-nonmatrix\">Backprop Derivations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the network depicted below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/backprop_net.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute $\\frac{\\partial \\mathcal{L}(\\theta) }{\\partial w_{i,j}^{(k)}}$ (the derivative of the loss w.r.t to the parameter $w_{i,j}^{(k)}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/backprop.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's derive the expression of  $\\frac{\\partial \\mathcal{L}(\\theta) }{\\partial w_{i,j}^{(k)}}$, assuming that\n",
    "- the derivatives of the loss w.r.t the preactivations of the $(k+1)$-th layer is given (*i.e.* $\\frac{\\partial \\mathcal{L}(\\theta) }{\\partial \\mathbf{q}_{k+1}}$ is given)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\frac{\\partial \\mathcal{L}(\\theta) }{\\partial w_{i,j}^{(k)}} &= \\big( \\frac{\\partial \\mathcal{L}(\\theta) }{\\partial \\mathbf{q}_{k}[i]} \\big) \\big( \\frac{\\partial \\mathbf{q}_{k}[i]}{\\partial w_{i,j}^{(k)}} \\big),& \\text{ weights interact directly with preactivations}\\\\\n",
    "        &= \\big( \\frac{\\partial \\mathcal{L}(\\theta) }{\\partial \\mathbf{z}_{k}[i]}\\big) \\big( \\frac{\\partial\\mathbf{z}_{k}[i] }{\\partial \\mathbf{q}_{k}[i]}\\big) \\big( \\frac{\\partial \\mathbf{q}_{k}[i]}{\\partial w_{i,j}^{(k)}} \\big), & \\text{ preactivations interacts with activations}\\\\\n",
    "        &= \\left[ \\sum_{p=1}^{h_{k+1}} \\big( \\frac{\\partial \\mathcal{L}(\\theta) }{\\partial \\mathbf{q}_{k+1}[p]}\\big) \\big( \\frac{\\partial \\mathbf{q}_{k+1}[p] }{\\partial \\mathbf{z}_{k}[i]}\\big) \\right] \\big( \\frac{\\partial\\mathbf{z}_{k}[i] }{\\partial \\mathbf{q}_{k}[i]}\\big) \\big( \\frac{\\partial \\mathbf{q}_{k}[i]}{\\partial w_{i,j}^{(k)}} \\big), & \\text{the } \\mathbf{z}_k\\text{'s interact with all the } \\mathbf{q}_{k+1}\\text{'s}\\\\\n",
    "        &= \\left[ \\sum_{p=1}^{h_{k+1}} \\big( \\frac{\\partial \\mathcal{L}(\\theta) }{\\partial \\mathbf{q}_{k+1}[p]}\\big) \\big(w_{p,i}^{(k+1)} \\big) \\right] \\big( \\frac{\\partial\\mathbf{z}_{k}[i] }{\\partial \\mathbf{q}_{k}[i]}\\big) \\big( \\frac{\\partial \\mathbf{q}_{k}[i]}{\\partial w_{i,j}^{(k)}} \\big), & \\text{since } \\mathbf{q}_{k+1}[p] = \\sum_{c=1}^{h_k} w_{p,c}^{(k+1)} \\mathbf{z}_{k}[c] + \\mathbf{b}_{k+1}[p]\\\\\n",
    "        &= \\left[ \\sum_{p=1}^{h_{k+1}} \\big( \\frac{\\partial \\mathcal{L}(\\theta) }{\\partial \\mathbf{q}_{k+1}[p]}\\big) \\big(w_{p,i}^{(k+1)} \\big) \\right] \\big( \\sigma_{k}'( \\mathbf{q}_{k}[i]) \\big) \\big( \\frac{\\partial \\mathbf{q}_{k}[i]}{\\partial w_{i,j}^{(k)}} \\big), & \\text{since } \\mathbf{z}_{k}[i] = \\sigma_k( \\mathbf{q}_{k}[i])\\\\\n",
    "        &= \\left[ \\sum_{p=1}^{h_{k+1}} \\big( \\frac{\\partial \\mathcal{L}(\\theta) }{\\partial \\mathbf{q}_{k+1}[p]}\\big) \\big(w_{p,i}^{(k+1)} \\big) \\right] \\big( \\sigma_{k}'( \\mathbf{q}_{k}[i]) \\big) \\big( \\mathbf{z}_{k-1}[j]  \\big). &\\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 <a class=\"anchor\" id=\"backprop-matrix\">Backprop Vectorized Form</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now rewrite the previous expressions in a more readable way using a matrix formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recall that $$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\nabla_{\\mathbf{W}_{k}} \\mathcal{L} (\\theta) &=\\left[ \\big(\\nabla_{\\mathbf{z}_{k}} \\mathcal{L} (\\theta)\\big) \\odot \\sigma_p'(\\mathbf{q}_{p}) \\right] \\big(\\mathbf{z}_{k-1}^T\\big).\\\\\n",
    "        &\\\\ \n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$\n",
    "Refer to section [2.2.1](#backprop-lastlayer-matrixform) for the complete proof. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's derive the complete vectorized expression of the gradients, assuming that $\\frac{\\partial \\mathcal{L}(\\theta) }{\\partial \\mathbf{q}_{k+1}}$  is given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\nabla_{\\mathbf{W}_{k}} \\mathcal{L} (\\theta) &=  \\left[ \\big(\\nabla_{\\mathbf{z}_{k}} \\mathcal{L} (\\theta)\\big) \\odot \\sigma_p'(\\mathbf{q}_{p}) \\right] \\big(\\mathbf{z}_{k-1}^T\\big)\\\\\n",
    "        &= \\left[  \\big( \\nabla_{\\mathbf{z}_{k}} \\mathbf{q}_{k+1} \\big) \\Big(\\big(\\nabla_{\\mathbf{q}_{k+1}} \\mathcal{L} (\\theta)\\big) \\odot \\sigma_p'(\\mathbf{q}_{p})  \\Big)\\right] \\big(\\mathbf{z}_{k-1}^T\\big), \\quad \\text{ since } \\mathbf{q}_k = \\mathbf{W}_k\\mathbf{z}_{k-1} + \\mathbf{b}_k \\\\\n",
    "        &= \\Pi_{p=K-1}^{k} \\left[  \\big( \\nabla_{\\mathbf{z}_{p}} \\mathbf{q}_{p+1} \\big) \\Big(\\big(\\nabla_{\\mathbf{q}_{p+1}} \\mathcal{L} (\\theta)\\big) \\odot \\sigma_p'(\\mathbf{q}_{p}) \\Big) \\right] \\big(\\mathbf{z}_{k-1}^T\\big), \\quad \\text{ reverse loop }\\\\\n",
    "        &=  \\Pi_{p=K-1}^{k} \\left[ \\big(\\mathbf{W}_{p+1}^T \\big) \\Big(\\big(\\nabla_{\\mathbf{q}_{p+1}} \\mathcal{L} (\\theta)\\big)  \\odot \\sigma_p'(\\mathbf{q}_{p})\\Big) \\right] \\big(\\mathbf{z}_{k-1}^T\\big), \\quad \\text{ final expression }\\\\\n",
    "        &\\\\\n",
    "        &\\\\\n",
    "        \\nabla_{\\mathbf{b}_{k}} \\mathcal{L} (\\theta) &=  \\Pi_{p=K-1}^{k} \\left[ \\big(\\mathbf{W}_{p+1}^T \\big) \\Big(\\big(\\nabla_{\\mathbf{q}_{p+1}} \\mathcal{L} (\\theta)\\big)  \\odot \\sigma_k'(\\mathbf{q}_{k}) \\Big)\\right] \\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 <a class=\"anchor\" id=\"backprop-algo\">Backprop Overall Algorithm (Vectorized form)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal is to have an algorithm to compute the derivatives of the $k$-th hidden parameters ( $\\mathbf{W}_{k}, \\mathbf{b}_k$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that\n",
    "$$\n",
    "\\nabla_{\\mathbf{q}_{k}} \\mathcal{L}(\\theta) = \\big(\\nabla_{\\mathbf{z}_{k}} \\mathcal{L} (\\theta) \\big)^T \\big(\\nabla_{\\mathbf{q}_{k}} \\mathbf{z}_{k}\\big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we compute the gradients of the hidden parameters\n",
    ">$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\nabla_{\\mathbf{W}_{k}} \\mathcal{L} (\\theta) &= \\big(\\nabla_{\\mathbf{q}_{k}} \\mathcal{L} (\\theta) \\big)^T \\big(\\nabla_{\\mathbf{W}_{k}} \\mathbf{q}_{k}\\big) &= \\big(\\nabla_{\\mathbf{q}_{k}} \\mathcal{L} (\\theta) \\big) \\big(\\mathbf{z}_{k-1}^T\\big)\\\\\n",
    "        \\nabla_{\\mathbf{b}_{k}} \\mathcal{L} (\\theta) &= \\big(\\nabla_{\\mathbf{q}_{k}} \\mathcal{L} (\\theta) \\big)^T \\big(\\nabla_{\\mathbf{b}_{k}} \\mathbf{q}_{k}\\big) &= \\big(\\nabla_{\\mathbf{q}_{k}} \\mathcal{L} (\\theta) \\big) \\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. we compute the gradient of the loss w.r.t the activations of the hidden layer below ($k-1$)\n",
    ">$$\n",
    "\\nabla_{\\mathbf{z}_{k-1}} \\mathcal{L} (\\theta) = \\big(\\nabla_{\\mathbf{z}_{k-1}} \\mathbf{z}_{k}\\big) \\big(\\nabla_{\\mathbf{q}_{k}} \\mathcal{L} (\\theta) \\big)   =  \\big(\\mathbf{W}_{k}^T\\big) \\big(\\nabla_{\\mathbf{q}_{k}} \\mathcal{L} (\\theta) \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. we compute the gradient of the loss w.r.t the pre-activations of the hidden layer below ($k-1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$$\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\nabla_{\\mathbf{q}_{k-1}} \\mathcal{L} (\\theta) &= \\big(\\nabla_{\\mathbf{z}_{k-1}} \\mathcal{L} (\\theta) \\big) \\odot \\sigma_{k-1}'(\\mathbf{q}_{k-1})\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Improving the Gradient Descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, the weights in the previous equation will remain unchanged when the loss plateaus, since the gradient will be null. \n",
    "Thus the necessity of using a **momentum**, $\\nu$, to continue the learning. The update rule becomes\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\nu^{(t+1)} &\\leftarrow   \\rho \\nu^{(t)} + (1 - \\rho)  \\nabla \\mathcal{L}(\\theta^{(t-1)}),\\\\\n",
    "        \\theta^{(t)} &\\leftarrow   \\theta^{(t-1)} - \\eta \\nu^{(t+1)},\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "where $\\rho \\in [0,1]$ is a hyper-parameter that indicates how much of the previous gradients we are keeping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Coding a Neural Network (long code, be ready!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "###############################################\n",
    "#########    LOSSES \n",
    "###############################################\n",
    "\n",
    "class MeanSquareLoss:\n",
    "    @staticmethod\n",
    "    def call(g, p):\n",
    "        \"\"\"\n",
    "            Compute the MSE between two arrays of the same size\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            g: (array-like)\n",
    "               ground-truths\n",
    "            p: (array-like)\n",
    "                predictions\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            d: (real number) the MSE value\n",
    "            \n",
    "        \"\"\"\n",
    "        g = g.reshape(p.shape)\n",
    "        return (.5/len(p))*np.sum((g - p)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad(g, p):\n",
    "        \"\"\"\n",
    "            Compute the GRADIENT of the MSE w.r.t the input ''p''\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            g: (array-like)\n",
    "               ground-truths\n",
    "            p: (array-like)\n",
    "                predictions\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            d: (array-like) the gradient\n",
    "            \n",
    "        \"\"\"\n",
    "        g = g.reshape(p.shape)\n",
    "        return (1./len(p))*(p - g)\n",
    "    \n",
    "class BinaryCrossEntropyLoss:\n",
    "    @staticmethod\n",
    "    def call(g, p):\n",
    "        \"\"\"\n",
    "            compute the binary cross-entropy between two arrays of the same size\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            g: (array-like)\n",
    "               ground-truths\n",
    "            p: (array-like)\n",
    "                predictions\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            d: (real number) the binary CE\n",
    "            \n",
    "        \"\"\"\n",
    "        g = g.reshape(p.shape)\n",
    "        return -(1./len(p))*np.sum(g*np.log(p) + (1. - g)*np.log(1. - p))\n",
    "    @staticmethod\n",
    "    def grad(g, p):\n",
    "        \"\"\"\n",
    "            compute the Gradient of the BCE w.r.t the parameter ''p''\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            g: (array-like)\n",
    "               ground-truths\n",
    "            p: (array-like)\n",
    "                predictions\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            d: (array-like) the gradient\n",
    "            \n",
    "        \"\"\"\n",
    "        g = g.reshape(p.shape)\n",
    "        return - (np.divide(g, np.maximum(p, 10e-5)) - np.divide(1. - g, 1. - p))\n",
    "\n",
    "class CategoricalCrossEntropy:\n",
    "    @staticmethod\n",
    "    def call(g, p):\n",
    "        g = g.reshape(p.shape)\n",
    "        return (-1/len(p))*np.sum(g*np.log(p))\n",
    "    @staticmethod\n",
    "    def grad(g, p):\n",
    "        g = g.reshape(p.shape)\n",
    "        return (-1/len(p))*np.divide(g, p)\n",
    "\n",
    "###############################################\n",
    "#########    ACTIVATIONS \n",
    "###############################################\n",
    "\n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def call(x):\n",
    "        \"\"\"\n",
    "            Compute a sigmoid activation on the input\n",
    "            A point-wise operation is performed for array-like inputs \n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            x: (array-like, or number)\n",
    "                inputs\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            d: (same shape with the inputs)\n",
    "        \"\"\"\n",
    "        return (1./(1. + np.exp(-x))).clip(10e-8,1-10e-8)\n",
    "    @staticmethod\n",
    "    def grad(x):\n",
    "        \"\"\"\n",
    "            Compute a gradient of the sigmoid activation w.r.t the the input\n",
    "            A point-wise operation is performed for array-like inputs \n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            x: (array-like, or number)\n",
    "                inputs\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            d: (same shape with the inputs)\n",
    "        \"\"\"\n",
    "        a = Sigmoid.call(x)\n",
    "        return a*(1. - a)\n",
    "\n",
    "class ReLU:\n",
    "    @staticmethod\n",
    "    def call(x):\n",
    "        \"\"\"\n",
    "            Compute a ReLU activation on the input\n",
    "            A point-wise operation is performed for array-like inputs \n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            x: (array-like, or number)\n",
    "                inputs\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            d: (same shape with the inputs)\n",
    "        \"\"\"\n",
    "        return np.maximum(0, x)\n",
    " \n",
    "    @staticmethod\n",
    "    def grad(x):\n",
    "        \"\"\"\n",
    "            Compute a gradient of the ReLU w.r.t the inputs\n",
    "            A point-wise operation is performed for array-like inputs \n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            x: (array-like, or number)\n",
    "                inputs\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            d: (same shape with the inputs)\n",
    "        \"\"\"\n",
    "        dx = np.ones_like(x)\n",
    "        dx[x <= 0] = 0\n",
    "        return dx\n",
    "\n",
    "class Softmax:\n",
    "    @staticmethod\n",
    "    def call(x):\n",
    "        \"\"\"\n",
    "            Compute a Softmax activation on the input\n",
    "            A point-wise operation is performed for array-like inputs \n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            x: (array-like, or number)\n",
    "                inputs\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            d: (same shape with the inputs)\n",
    "        \"\"\"\n",
    "        v = np.exp(x) \n",
    "        return (v/v.sum(1)[:,None]).clip(10e-8,1-10e-8)\n",
    "    @staticmethod\n",
    "    def grad(x):\n",
    "        \"\"\"\n",
    "            Compute a gradient of the softmax activation w.r.t the the input\n",
    "            A point-wise operation is performed for array-like inputs \n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            x: (array-like, or number)\n",
    "                inputs\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            d: (same shape with the inputs)\n",
    "        \"\"\"\n",
    "        v = Softmax.call(x)\n",
    "        return v - v**2\n",
    "\n",
    "    \n",
    "class NoActivation:\n",
    "    @staticmethod\n",
    "    def call(x):\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def grad(x):\n",
    "        return x\n",
    "\n",
    "###############################################\n",
    "#########    OPTIMIZERS \n",
    "###############################################\n",
    "\n",
    "class SGDOptimizer:\n",
    "    def __init__(self, lr=0.1, rho=0.):\n",
    "        \"\"\"\n",
    "            A class containing the Stochastic Gradient Descent algorithm \n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            lr: (float)\n",
    "                the learning rate\n",
    "            rho: (float)\n",
    "                the value of the momentum\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.rho = rho\n",
    "        \n",
    "        # initializing the vectors of forwards outputs\n",
    "        self.outputs = []\n",
    "        # initializing the vectors of backwards outputs (the gradients)\n",
    "        self.grads = []\n",
    "    \n",
    "    def update_grads(self, grads, k):\n",
    "        \"\"\"\n",
    "            Compute the changes to the gradients\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            grads: (list)\n",
    "                list of gradients of a certain layer \n",
    "            k: (int)\n",
    "                index of the layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # momentum update\n",
    "        if self.grads[k] != []:\n",
    "            self.grads[k] = [self.rho*self.grads[k][i] + (1. - self.rho)*grads[i] for i in range(len(grads))]\n",
    "        else:\n",
    "            self.grads[k] = [(1. - self.rho)*grads[i] for i in range(len(grads))]\n",
    "            \n",
    "        self.grads[k] = [np.clip(grad, -0.5, 0.5) for grad in self.grads[k] ]\n",
    "        self.grads[k] = [grad/np.linalg.norm(grad) for grad in self.grads[k] ]\n",
    "        \n",
    "        # return the final change to make the parameters\n",
    "        return [-self.lr * grad for grad in self.grads[k]]\n",
    "    \n",
    "    def apply_gradients(self, layers, grad_loss):\n",
    "        \"\"\"\n",
    "            Compute the gradients of the loss w.r.t each layers' parameters \n",
    "            and update the parameters. \n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            layers: (list)\n",
    "                list of layers (the inputs are in layers[0]) \n",
    "            grad_loss: (array-like)\n",
    "                gradient of the loss w.r.t the final output\n",
    "        \"\"\"\n",
    "        K = len(layers)-1 \n",
    "        for k in range(K, -1, -1): \n",
    "            l_grads, grad_loss = layers[k].compute_gradients(self.outputs[k], grad_loss )\n",
    "            layers[k].update_params(self.update_grads(l_grads, k)) \n",
    "\n",
    "\n",
    "class RMSPropOptimizer:\n",
    "    def __init__(self, lr=0.1, rho=0.):\n",
    "        \"\"\"\n",
    "            A class containing the Stochastic Gradient Descent algorithm \n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            lr: (float)\n",
    "                the learning rate\n",
    "            rho: (float)\n",
    "                the value of the momentum\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.rho = rho\n",
    "        \n",
    "        # initializing the vectors of forwards outputs\n",
    "        self.outputs = []\n",
    "        # initializing the vectors of backwards outputs (the gradients)\n",
    "        self.grads = []\n",
    "    \n",
    "    def update_grads(self, grads, k):\n",
    "        \"\"\"\n",
    "            Compute the changes to the gradients\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            grads: (list)\n",
    "                list of gradients of a certain layer \n",
    "            k: (int)\n",
    "                index of the layer\n",
    "        \"\"\"\n",
    "        # momentum update\n",
    "        if self.grads[k] != []:\n",
    "            self.grads[k] = [self.rho*self.grads[k][i] + (1. - self.rho)*grads[i]**2 for i in range(len(grads))]\n",
    "        else:\n",
    "            self.grads[k] = [(1. - self.rho)*grads[i]**2 for i in range(len(grads))]\n",
    "            \n",
    "        self.grads[k] = [np.clip(grad, -0.5, 0.5) for grad in self.grads[k] ]\n",
    "        self.grads[k] = [grad/np.linalg.norm(grad) for grad in self.grads[k] ]\n",
    "        \n",
    "        # return the final change to make the parameters\n",
    "        return [-self.lr * grads[i] * np.power(self.grads[k][i] + 1e-6, -0.5) for i in range(len(grads))]\n",
    "    \n",
    "    def apply_gradients(self, layers, grad_loss):\n",
    "        \"\"\"\n",
    "            Compute the gradients of the loss w.r.t each layers' parameters \n",
    "            and update the parameters. \n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            layers: (list)\n",
    "                list of layers (the inputs are in layers[0]) \n",
    "            grad_loss: (array-like)\n",
    "                gradient of the loss w.r.t the final output\n",
    "        \"\"\"\n",
    "        K = len(layers)-1 \n",
    "        for k in range(K, -1, -1): \n",
    "            l_grads, grad_loss = layers[k].compute_gradients(self.outputs[k], grad_loss )\n",
    "            layers[k].update_params(self.update_grads(l_grads, k)) \n",
    "\n",
    "        \n",
    "###############################################\n",
    "#########    LAYERS \n",
    "###############################################\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_neurons, activation=\"sigmoid\", l2_regul=0.0):\n",
    "        \"\"\"\n",
    "            A Dense layer class (Fully-connected layer)\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            n_neurons: (int)\n",
    "                number of neurons in the layer\n",
    "            activation: (string)\n",
    "                activation function to use\n",
    "        \"\"\"\n",
    "        self.N = n_neurons\n",
    "        self.l2_regul = l2_regul\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activ_func = Sigmoid\n",
    "        elif activation == 'relu':\n",
    "            self.activ_func = ReLU\n",
    "        elif activation == \"softmax\":\n",
    "            self.activ_func = Softmax\n",
    "        else:\n",
    "            self.activ_func = NoActivation\n",
    "            \n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "            Initialize the layer by creating the parameters\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            input_shape: (list or tuple or array)\n",
    "                shape of the inputs. Must be of the form (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        #self.W = 0.01 * np.random.randn(self.N, input_shape[1])\n",
    "        #self.b = 0.01 * np.random.randn(self.N)\n",
    "        \n",
    "        # Glorot Uniform initialization\n",
    "        l = np.sqrt(6./(self.N + input_shape[1]))\n",
    "        self.W = np.random.uniform(low=-l, high=l, size=(self.N, input_shape[1]))\n",
    "        self.b =  np.random.uniform(low=-l, high=l, size=self.N)\n",
    "        \n",
    "    def preactiv(self, inputs):\n",
    "        \"\"\"\n",
    "            Computes the preactivation of the inputs\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            inputs: (array-like), shape=(batch_size, input_dim)\n",
    "                the inputs to the layer\n",
    "        \"\"\"\n",
    "        return np.dot( inputs, self.W.T) +  self.b\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "            Computes the preactivation and apply the activation function\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            inputs: (array-like), shape=(batch_size, input_dim)\n",
    "                the inputs to the layer\n",
    "        \"\"\"\n",
    "        return self.activ_func.call(self.preactiv(inputs))\n",
    "    \n",
    "    def get_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            Returns the output shapte of the layer\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            input_shape: (list or tuple or array)\n",
    "                shape of the inputs. Must be of the form (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        out_shape = list(input_shape)\n",
    "        out_shape[-1] = self.N\n",
    "        return out_shape\n",
    "    \n",
    "    def compute_gradients(self, inputs, back_grads): \n",
    "        \"\"\"\n",
    "            Computes the gradients of this layers' parameters\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            inputs: (array-like), shape=(batch_size, input_dim)\n",
    "                the inputs to the layer\n",
    "            back_grads: (array-like), shape=(batch_size, self.N)\n",
    "                gradients backpropagated from the next layer\n",
    "        \"\"\"\n",
    "        m = inputs.shape[0]\n",
    "        dzk_dqk = self.activ_func.grad(self.preactiv(inputs))\n",
    "        back_grads = back_grads * dzk_dqk\n",
    "        \n",
    "        grad_w = np.dot(back_grads.T, inputs) / m + self.l2_regul*self.W\n",
    "        grad_b = np.sum(back_grads, axis=0) / m\n",
    "         \n",
    "        back_grads = np.dot(back_grads, self.W) \n",
    "        \n",
    "        return [grad_w, grad_b], back_grads\n",
    "    \n",
    "    def update_params(self, grads):\n",
    "        \"\"\"\n",
    "            Updates the parameters using the changes provided by the Optimizer class\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            grads: (list)\n",
    "                gradients of each parameter of this layer \n",
    "        \"\"\"\n",
    "        self.W += grads[0]\n",
    "        self.b += grads[1]\n",
    "\n",
    "###############################################\n",
    "#########    MODEL \n",
    "###############################################\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, input_shape):\n",
    "        \"\"\"\n",
    "            A generic Neural Network class\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            input_shape: (list or tuple or array)\n",
    "                shape of the inputs. Must be of the form (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.layers = []\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def compile_model(self, loss, optimizer):\n",
    "        self.loss = loss\n",
    "        self.opt = optimizer\n",
    "        self.n_layers = len(self.layers)\n",
    "        \n",
    "        input_shape = self.input_shape\n",
    "        for l in self.layers:\n",
    "            l.build(input_shape)\n",
    "            input_shape = l.get_output_shape(input_shape)\n",
    "            self.opt.outputs.append([])\n",
    "            self.opt.grads.append([])\n",
    "            \n",
    "        self.opt.outputs.append([])\n",
    "        self.opt.grads.append([])\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.opt.outputs[0] = X\n",
    "        for i in range(1, self.n_layers+1):\n",
    "            self.opt.outputs[i]  = self.layers[i-1].call(self.opt.outputs[i-1])\n",
    "        return self.opt.outputs[-1]\n",
    "    \n",
    "    def backward(self, y, zK):\n",
    "        grad_loss = self.loss.grad(y, zK)\n",
    "        self.opt.apply_gradients(self.layers, grad_loss)\n",
    "        return grad_loss\n",
    "    \n",
    "    def fit(self, X, y, n_iter=100, batch_size=32, verbose=0):\n",
    "        \"\"\"\n",
    "            Training the model\n",
    "            \n",
    "            X: (np-array) shape=(n_samples x n_feats)\n",
    "                data matrix\n",
    "            y: (np-array) shape=( n_samples)\n",
    "                targets \n",
    "            n_iter: (int) \n",
    "                number of iterations\n",
    "        \"\"\"\n",
    "        self.losses = []\n",
    "        for e in range(n_iter):\n",
    "            batch_loss = 0.\n",
    "            for i in range(len(X)//batch_size):\n",
    "                output = self.forward(X[i:i+batch_size])\n",
    "                batch_loss += self.loss.call(y[i:i+batch_size], output)\n",
    "                self.backward(y[i:i+batch_size], output)\n",
    "            self.losses.append(batch_loss / (len(X)//batch_size))\n",
    "            if verbose:\n",
    "                print('Epoch [{}/{}], loss {:.8f}'.format(e, n_iter, self.losses[-1]))\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            Predicting the discrete labels\n",
    "            \n",
    "            X: (np-array) shape=(n_samples x n_feats)\n",
    "                data matrix \n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "###############################################\n",
    "#########    UTILS \n",
    "###############################################\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    \"\"\"\n",
    "        Convert a vector of targets [1, 2, 0] to its one-hot encoded \n",
    "        version, which is the matrix [[0, 1, 0], [0, 0, 1], [1, 0, 0]].\n",
    "\n",
    "        y: (np-array) shape=( n_samples)\n",
    "            targets \n",
    "    \"\"\"\n",
    "    v = np.zeros((y.size, len(np.unique(y))))\n",
    "    v[np.arange(y.size), y.astype(int)] = 1\n",
    "    return v\n",
    "\n",
    "class CastMulticlassToBinaryPrediction:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.model.predict(X), 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Example on toy datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_circles, make_moons\n",
    "import pylab as plt\n",
    "#X, y = make_blobs(n_samples=50, centers=2)\n",
    "X, y = make_circles(100, factor=.2, noise=.1)\n",
    "X, y = make_moons(noise=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Designing an ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mlp = NN(X.shape)\n",
    "my_mlp.add_layer(DenseLayer(32, activation=\"relu\"))\n",
    "my_mlp.add_layer(DenseLayer(64, activation=\"relu\")) \n",
    "my_mlp.add_layer(DenseLayer(128, activation=\"relu\")) \n",
    "my_mlp.add_layer(DenseLayer(1, activation=\"sigmoid\"))\n",
    "my_mlp.compile_model(BinaryCrossEntropyLoss, RMSPropOptimizer(lr=.01, rho=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mlp.fit(X, y, 300, batch_size=len(X))\n",
    "print(\"score = \", np.mean(np.squeeze(my_mlp.predict(X) >=0.5).astype(float) == y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.4 Plotting the losses curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(my_mlp.losses)), my_mlp.losses);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.5 Plotting the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_decision(X, y, my_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Example on Multiclass toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "# create the dataset\n",
    "X_mclr, y_mclr = make_classification(n_samples=200, n_classes=4, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=0, n_clusters_per_class=1, class_sep=1.5)\n",
    "# convert the targets to one-hot encoded vectors\n",
    "y_mclr = one_hot_encode(y_mclr)\n",
    "\n",
    "# design a multi-output network and compile it\n",
    "my_mlp_mc = NN(X.shape)\n",
    "my_mlp_mc.add_layer(DenseLayer(32, activation=\"relu\"))\n",
    "my_mlp_mc.add_layer(DenseLayer(64, activation=\"relu\")) \n",
    "my_mlp_mc.add_layer(DenseLayer(4, activation=\"softmax\"))\n",
    "my_mlp_mc.compile_model(CategoricalCrossEntropy, RMSPropOptimizer(lr=0.005, rho=0.5))\n",
    "\n",
    "# training the model\n",
    "my_mlp_mc.fit(X_mclr, y_mclr, n_iter=100)\n",
    "# plotting the decision boundaries\n",
    "print_decision(X_mclr, \n",
    "               np.argmax(y_mclr, 1),\n",
    "               CastMulticlassToBinaryPrediction(my_mlp_mc),\n",
    "               \"Decision Boundary of our Multiclass MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Example on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# Loading the images\n",
    "if not os.path.exists(\"mnist_784_data.pkl\"):\n",
    "    X_mnist, y_mnist = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "    y_mnist = one_hot_encode(y_mnist.astype(int))\n",
    "    with open('mnist_784_data.pkl','wb') as f:\n",
    "         pickle.dump([X_mnist, y_mnist], f) \n",
    "else:\n",
    "    with open('mnist_784_data.pkl','rb') as f:\n",
    "        X_mnist, y_mnist = pickle.load(f)\n",
    "\n",
    "# shuffling part\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X_mnist.shape[0])\n",
    "X_mnist = X_mnist[permutation]\n",
    "y_mnist = y_mnist[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "# data splitting\n",
    "X_train_mnist, X_test_mnist, y_train_mnist, y_test_mnist = train_test_split(\n",
    "    X_mnist, y_mnist, train_size=5000, test_size=10000)\n",
    "\n",
    "# Normalization\n",
    "scaler = StandardScaler()\n",
    "X_train_mnist = scaler.fit_transform(X_train_mnist)\n",
    "X_test_mnist = scaler.transform(X_test_mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mlp = NN(X_train_mnist.shape)\n",
    "my_mlp.add_layer(DenseLayer(32, activation=\"relu\")) \n",
    "my_mlp.add_layer(DenseLayer(64, activation=\"relu\")) \n",
    "my_mlp.add_layer(DenseLayer(128, activation=\"relu\"))  \n",
    "my_mlp.add_layer(DenseLayer(10, activation=\"softmax\"))\n",
    "my_mlp.compile_model(CategoricalCrossEntropy, RMSPropOptimizer(.0002, rho=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_mlp.fit(X_train_mnist[:1000], y_train_mnist[:1000], n_iter=150, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(my_mlp.losses)), my_mlp.losses)\n",
    "plt.title(\"Learning curve MNIST\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5)) \n",
    "for i in range(10):\n",
    "    pred_plot = plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    pred = np.argmax(my_mlp.predict(X_test_mnist[i:i+1]), 1)\n",
    "    \n",
    "    pred_plot.imshow(X_test_mnist[i].reshape(28, 28), interpolation='nearest', \n",
    "                     cmap=plt.cm.gray)\n",
    "    pred_plot.set_xticks(())\n",
    "    pred_plot.set_yticks(())\n",
    "    pred_plot.set_xlabel('Pred: %s, True: %s' %(pred, np.argmax(y_test_mnist[i]) ))\n",
    "plt.suptitle('Classification vector for...')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefs(coef, title='Weights vector for...'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    scale = np.abs(coef).max()\n",
    "    for i in range(10):\n",
    "        l1_plot = plt.subplot(2, 5, i + 1)\n",
    "        l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',\n",
    "                       cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
    "        l1_plot.set_xticks(())\n",
    "        l1_plot.set_yticks(())\n",
    "        l1_plot.set_xlabel('Class %i' % i)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coefs(my_mlp.layers[0].W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a class=\"anchor\" id=\"cnn\">Convolutional Models</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1. Definition\n",
    "\n",
    "\n",
    "Convolutional neural networks (CNNs) are particular types of FNN -- introduced in this [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf) by \n",
    "[Yann LeCun](https://fr.wikipedia.org/wiki/Yann_Le_Cun) and collaborators-- that takes into account the structure/topology of their inputs in the processing.\n",
    "\n",
    "They tackle the following points that are ignored in standard FNNs.\n",
    "\n",
    "- **Dealing with very high-dimensional inputs**. For instance, given an RGB image of size $200\\times200\\times3$,\n",
    " the weights matrix between the input and the first hidden layer, of dimension $h_1$, is of size $120000\\times h_1$.\n",
    " As a result, not only will the number of parameters explode, but the computation will also be  time-consuming.\n",
    "- **Exploiting the input topology** (spatial structure), $e.g.$ $2$D or $3$D images. It can be shown (**see** dictionary\n",
    " learning models, Fourier transforms) that an image can be written as linear combination of patterns. Accordingly, instead of \n",
    " having a large weight matrix on the entire image, it may be better to have small ones that look for certain patterns.\n",
    "- **Building invariance to certain variations**, $e.g.$ translation, illumination, etc. In an image classification\n",
    " task, the output should be remain the same, to extent, after small transformation on the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, the CNNs incorporate the following techniques.\n",
    "\n",
    "- **Local connectivity**: It removes the cumbersome and time-consuming fully connection between two\n",
    " layers by local connection. That is, each layer is divided into **feature maps** and the weights\n",
    " are applied on local regions of the input called **receptive fields**.\n",
    "- **Parameter sharing**: Going further, all neurons of a feature map are forced to share\n",
    " the same weights. Consequently, in a feature map, the neurons look for the same pattern but at \n",
    " different locations in the input. The $i$-th feature map of the $l$-layer can, therefore, be seen as the result of \n",
    " a **cross-correlation** operation between its weights $\\mathbf{W}_l^i$ and its input $\\mathbf{z}_l$. \n",
    " By language abuse, this operation is called convolution thus the name **convolutional neural networks**.\n",
    "- **Pooling/sub-sampling**: This point aims at reducing the dimension of a layer by aggregating\n",
    " its feature maps structurally. Also, the aggregation can be done so as to make the output invariant \n",
    " to small translation by taking the maximum in a sliding-window manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Graphical derivation from a standard ANN to a Convolution based NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following MLP.\n",
    "\n",
    "<img src=\"images/ann2.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Forcing local connection: we set some elements of the weight vectors to $0$. The resulting MLP is as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/localconnect.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then, we tie some parameters together so they can share the same value. \n",
    "> In the figure below, the tied parameters share the same color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/weightshare.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that, the operation in the first layer can be seen as a convolution plus the bias term, as depicted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/weightshare3.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Convolution and Pooling layers in practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolution layer usually consists of the following:\n",
    "- $f$ **filters** (or kernels), usually squared filters are used,\n",
    "- each filters is of size $k\\times k \\times c$, where $c$ is the number of channels in the previous layers' output,\n",
    "- $p$ a **padding** value, in case the inputs are padded before convolution ($p=0$ by default),\n",
    "- $s$ a **stride** value, in case the convolution are performed with a stride ($s=1$ by default) .\n",
    "\n",
    "\n",
    "The figure below present two convolution layers with $p=0$ and $s=1$\n",
    "<img src=\"images/conv_layer.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Pooling** layer is a similar to the convolution one\n",
    "- except there is **no convolution**\n",
    "- the filter is slided across the image with a certain stride\n",
    "- **we take the maximum or average in the spacial dimension and keep the depth** (*see* the figure below) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pooling.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $g_k(\\cdot;  \\theta_k)$ be the function representing the $k$-th convolution layers, we will use the following notations:\n",
    "- $f_k$ is the number of filter maps in the $k$-th layer,\n",
    "- $h_k$ is the size of the filter maps in the $k$-th layer (*i.e.* squared filter maps),\n",
    "- $(u_k, v_k)$ is the height and width of the output of the $k$-th layer,\n",
    "\n",
    "\n",
    "- $g_k(\\cdot; \\theta) = \\mathbf{Z}_k \\in \\mathbb{R}^{u_{k} \\times v_k \\times f_{k}}$, the output of the $k$-th layer,\n",
    "\n",
    "\n",
    "- $\\mathbf{W}_k=\\{\\mathbf{W}_k^{(p)}\\}_{p=1}^{f_k}$ is the $k$-th **filter bank**, with $ \\mathbf{W}_k^{(p)} \\in \\mathbb{R}^{h_{k} \\times h_k \\times f_{k-1}} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Definition of the convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The $p$-th output map $\\mathbf{Z}_k^{(p)} \\in \\mathbb{R}^{u_{k} \\times v_k} $ is computed as follows\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "\\mathbf{Z}_k^{(p)} &=  \\sigma_k \\big(\\mathbf{Q}_k^{(p)} \\big) = \\sigma_k \\left( \\sum_{c=1}^{f_k}  \\mathbf{\\tilde{Z}}_{k-1}^{(c)} * \\mathbf{W}_{k}^{(p)}[\\cdot, \\cdot, c] \\right)  .\\\\\n",
    "    &\\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\mathbf{\\tilde{Z}} = \n",
    "\\begin{cases}\n",
    "\\mathbf{Z} & \\text{ if no padding is used},\\\\\n",
    "\\text{ zero-padded } \\mathbf{Z} & \\text{ if padding is used}.\n",
    "\\end{cases}$ \n",
    "\n",
    "> Therefore, the value of the  the $i$-th row and $j$-th column of $\\mathbf{Z}_k^{(p)}$ is given by  \n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "\\mathbf{Z}_k^{(p)}[i,j] &= \\sigma_k \\left( \\sum_{\\gamma=1}^{f_k} \\left(\\sum_{\\alpha, \\beta = 1}^{h_k} \\mathbf{\\tilde{Z}}_{k-1}^{(\\gamma)}[i+\\alpha, j+\\beta] \\mathbf{W}_{k}^{(p)}[\\alpha, \\beta, \\gamma] \\right)\\right).\\\\\n",
    "    &\\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Gradients in a  convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the gradient of loss w.r.t the preactivation of the $p$-th feature map of the $k$-th layer $\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{Q}_k^{(p)}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the previously seen backpropagation algorithm to update the parameters by going through the network from the output to the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we compute the gradients of the hidden parameters\n",
    ">$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "    &\\\\\n",
    "        \\frac{\\partial \\mathcal{L}(\\theta)}{\\mathbf{W}_k^{(p)}[\\alpha, \\beta, \\gamma]} &= \\sum_{i=1}^{u_k}\\sum_{j=1}^{v_k} \\left(\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{Q}_k^{(p)}[i,j]} \\right) \\left(\\frac{\\partial \\mathbf{Q}_k^{(p)}[i,j]}{\\mathbf{W}_k^{(p)}[\\alpha, \\beta, \\gamma]}\\right),\\\\\n",
    "        &= \\sum_{i=1}^{u_k}\\sum_{j=1}^{v_k} \\left(\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{Q}_k^{(p)}[i,j]} \\right) \\left(\\mathbf{\\tilde{Z}}_{k-1}^{(\\gamma)}[i+\\alpha, j+\\beta]\\right).\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. we compute the gradient of the loss w.r.t the activations of the hidden layer below ($k-1$)\n",
    ">$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "    &\\\\\n",
    "    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{\\tilde{Z}}_{k-1}^{(\\gamma)}[a,b]} &=  \\sum_{\\kappa = 1}^{f_k} \\sum_{i=1}^{u_k}\\sum_{j=1}^{v_k} \\left(\\left(\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{Q}_{k}^{(\\kappa)}[i,j]} \\right)\n",
    "    \\left(\\frac{\\partial \\mathbf{Q}_{k}^{(\\kappa)}[i,j]}{\\partial \\mathbf{\\tilde{Z}}_{k-1}^{(\\gamma)}[a,b]} \\right) \\right), \\\\\n",
    "    &= \\sum_{\\kappa = 1}^{f_k} \\sum_{i=1}^{u_k}\\sum_{j=1}^{v_k} \\left(\\left( \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{Q}_{k}^{(\\kappa)}[i,j]} \\right) \\left(  \\mathbf{W}_{k}^{(\\kappa)}[a-i, b-j, \\gamma]  \\right) \\right).\\\\\n",
    "    &\\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$ \n",
    "By using the substitution  $\\alpha = a - i$ and $\\beta = b - j$ we get\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "    &\\\\\n",
    "    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{\\tilde{Z}}_{k-1}^{(\\gamma)}[a,b]} &=  \\sum_{\\kappa = 1}^{f_k} \\sum_{\\alpha=a-1}^{a-u_k}\\sum_{\\beta=b - 1}^{b - v_k} \\left(\\left( \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{Q}_{k}^{(\\kappa)}[a-\\alpha,b-\\beta]} \\right) \\left(  \\mathbf{W}_{k}^{(\\kappa)}[\\alpha, \\beta, \\gamma]  \\right) \\right),\\\\\n",
    "    &\\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$ \n",
    "Therefore, \n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "    &\\\\\n",
    "    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{\\tilde{Z}}_{k-1}^{(\\gamma)}} &=   \\sum_{\\kappa=1}^{f_k} \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{Q}_{k}^{(\\kappa)} } * \\mathbf{W}_{k}^{(\\kappa)}[\\cdot, \\cdot, \\gamma].\\\\\n",
    "    &\\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. we compute the gradient of the loss w.r.t the pre-activations of the hidden layer below ($k-1$)\n",
    ">$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "    &\\\\\n",
    "    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{Q}_{k-1}^{(p)}[i,j]} &= \\left(\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{Z}_{k-1}^{(p)}[i,j]}\\right) \\left(\\frac{\\partial \\mathbf{Z}_{k-1}^{(p)}[i,j]}{\\partial \\mathbf{Q}_{k-1}^{(p)}[i,j]}\\right),\\\\\n",
    "    &= \\left(\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{Z}_{k-1}^{(p)}[i,j]}\\right) \\left( \\sigma_{k-1}' \\big(   \\mathbf{Q}_{k-1}^{(p)}[i,j] \\big) \\right),\\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Gradients in a  convolution layer (vectorized form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the gradient of loss w.r.t the preactivation of the $p$-th feature map of the $k$-th layer $\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{Q}_k^{(p)}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Example: The LeNet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet is a CNN architect proposed for handwritten digit classification. It consists of:\n",
    "- $5$ convolutional layers,\n",
    "- $2$ fully connected layers,\n",
    "- $2$ subsampling layers,\n",
    "- a $10$-dimensional output vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is  depicted in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/LeNet_Original_Image.jpg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Coding a Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.signal import convolve\n",
    "\n",
    "class Conv2DLayer:\n",
    "    def __init__(self, n_filters, size=(3,3), padding=\"valid\", activation=\"sigmoid\", \n",
    "                 initializer=\"xavier\", l2_regul=0.0):\n",
    "        \"\"\"\n",
    "            A Conv layer class\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            n_filters: (int)\n",
    "                number of kernels in the layer\n",
    "            size: (list or tuple or array)\n",
    "                height and width of the kernels\n",
    "            padding: (string)\n",
    "                type of padding (same or valid)\n",
    "            activation: (string)\n",
    "                activation function to use\n",
    "        \"\"\"\n",
    "        self.N = n_filters\n",
    "        self.k_size = size\n",
    "        self.l2_regul = l2_regul\n",
    "        self.initializer = initializer\n",
    "        self.padding = padding\n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            self.activ_func = Sigmoid\n",
    "        elif activation == 'relu':\n",
    "            self.activ_func = ReLU\n",
    "        elif activation == \"softmax\":\n",
    "            self.activ_func = Softmax\n",
    "        else:\n",
    "            self.activ_func = NoActivation\n",
    "            \n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "            Initialize the layer by creating the parameters\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            input_shape: (list or tuple or array)\n",
    "                shape of the inputs. Must be of the form (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        k_shape = (self.N, self.k_size[0], self.k_size[1], input_shape[-1])\n",
    "        if self.initializer == \"rand\":\n",
    "            self.W = 0.01 * np.random.randn(*k_shape)\n",
    "            self.b = 0.01 * np.random.randn(self.N)\n",
    "        else:\n",
    "            # Glorot Uniform initialization\n",
    "            l = np.sqrt(6./(self.N + input_shape[1]))\n",
    "            self.W = np.random.uniform(low=-l, high=l, size=k_shape)\n",
    "            self.b =  np.random.uniform(low=-l, high=l, size=self.N)\n",
    "        \n",
    "        self.n_pad = (0,0) if self.padding == \"valid\" else (self.k_size[0]//2, self.k_size[1]//2)\n",
    "        \n",
    "    def preactiv(self, inputs):\n",
    "        \"\"\"\n",
    "            Computes the preactivation of the inputs\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            inputs: (array-like), shape=(batch_size, input_dim)\n",
    "                the inputs to the layer\n",
    "        \"\"\"\n",
    "        return np.array([np.array([convolve(inputs[i], self.W[j,::-1, ::-1, :], mode=self.padding)\n",
    "                         for j in range(self.N)]).sum(-1).transpose(1,2,0)\n",
    "                        for i in range(len(inputs))])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "            Computes the preactivation and apply the activation function\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            inputs: (array-like), shape=(batch_size, input_dim)\n",
    "                the inputs to the layer\n",
    "        \"\"\"\n",
    "        return  self.activ_func.call(self.preactiv(inputs))\n",
    "    \n",
    "    def get_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            Returns the output shapte of the layer\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            input_shape: (list or tuple or array)\n",
    "                shape of the inputs. Must be of the form (batch_size, height, width, n_channels)\n",
    "        \"\"\"\n",
    "        out_shape = list(input_shape)\n",
    "        \n",
    "        out_shape[-1] = self.N\n",
    "        out_shape[1] = (input_shape[1] + 2*self.n_pad[0] - self.k_size[0]) + 1\n",
    "        out_shape[2] = (input_shape[2] + 2*self.n_pad[1] - self.k_size[1]) + 1\n",
    "        return out_shape\n",
    "    \n",
    "    def compute_gradients(self, inputs, back_grads): \n",
    "        \"\"\"\n",
    "            Computes the gradients of this layers' parameters\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            inputs: (array-like), shape=(batch_size, input_dim)\n",
    "                the inputs to the layer\n",
    "            back_grads: (array-like), shape=(batch_size, self.N)\n",
    "                gradients backpropagated from the next layer\n",
    "        \"\"\"\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        inputs = np.pad(inputs, ((0,), (0,), (self.n_pad[0],), (self.n_pad[1], )), 'constant')\n",
    "        back_grads = back_grads*self.activ_func.grad(back_grads)\n",
    "        \n",
    "        pg_1, pg_2 = back_grads.shape[1], back_grads.shape[2]\n",
    "        grad_w = np.array([[[\n",
    "                   (inputs[:, a:a + pg_1 ,b:b + pg_2, :]*back_grads[:,:,:,f:f+1]).sum((0, 1, 2))\n",
    "                    for b in range(self.k_size[1])]\n",
    "                for a in range(self.k_size[0])] \n",
    "                for f in range(self.N)])\n",
    "        \n",
    "        grad_b = 0\n",
    "        \n",
    "        back_grads = np.array([np.array([convolve(back_grads[i], self.W[j,::-1, ::-1, :], mode=\"same\")\n",
    "                         for j in range(self.N)]).sum(-1).transpose(1,2,0)\n",
    "                        for i in range(len(inputs))])\n",
    "        \n",
    "        return [grad_w, grad_b], back_grads\n",
    "    \n",
    "    def update_params(self, grads):\n",
    "        \"\"\"\n",
    "            Updates the parameters using the changes provided by the Optimizer class\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            grads: (list)\n",
    "                gradients of each parameter of this layer \n",
    "        \"\"\"\n",
    "        self.W += grads[0]\n",
    "        self.b += grads[1]\n",
    "\n",
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            A Flatten layer class\n",
    "            \n",
    "            Params\n",
    "            ------- \n",
    "        \"\"\" \n",
    "        pass\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "            Initialize the layer by creating the parameters\n",
    "            \n",
    "            **Not used by the FlattenLayer**\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            input_shape: (list or tuple or array)\n",
    "                shape of the inputs. Must be of the form (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "            Computes the preactivation and apply the activation function\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            inputs: (array-like), shape=(batch_size, input_dim)\n",
    "                the inputs to the layer\n",
    "        \"\"\"\n",
    "        return  inputs.reshape(inputs.shape[0], -1)\n",
    "    \n",
    "    def get_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            Returns the output shapte of the layer\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            input_shape: (list or tuple or array)\n",
    "                shape of the inputs. Must be of the form (batch_size, height, width, n_channels)\n",
    "        \"\"\"\n",
    "        in_shape = list(input_shape)\n",
    "        out_shape = [in_shape[0], np.prod(in_shape[1:])]\n",
    "        return out_shape\n",
    "    \n",
    "    def compute_gradients(self, inputs, back_grads): \n",
    "        \"\"\"\n",
    "            Computes the gradients of this layers' parameters\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            inputs: (array-like), shape=(batch_size, input_dim)\n",
    "                the inputs to the layer\n",
    "            back_grads: (array-like), shape=(batch_size, self.N)\n",
    "                gradients backpropagated from the next layer\n",
    "        \"\"\"\n",
    "        return [0], back_grads.reshape(inputs.shape)\n",
    "    \n",
    "    def update_params(self, grads):\n",
    "        \"\"\"\n",
    "            Updates the parameters using the changes provided by the Optimizer class\n",
    "            \n",
    "            **Not used by the FlattenLayer**\n",
    "            \n",
    "            Params\n",
    "            -------\n",
    "            grads: (list)\n",
    "                gradients of each parameter of this layer \n",
    "        \"\"\" \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Trying our ConvLayer on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cnn = NN((10, 28, 28, 1))\n",
    "my_cnn.add_layer(Conv2DLayer(10, activation=\"relu\"))\n",
    "my_cnn.add_layer(Conv2DLayer(20, activation=\"relu\"))\n",
    "my_cnn.add_layer(FlattenLayer())\n",
    "my_cnn.add_layer(DenseLayer(10, activation=\"softmax\"))\n",
    "my_cnn.compile_model(CategoricalCrossEntropy, RMSPropOptimizer(0.0005, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cnn.fit(X_train_mnist[:200].reshape(-1, 28,28,1), y_train_mnist[:200], n_iter=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(my_cnn.predict(X_train_mnist[:200].reshape(-1, 28,28,1)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(np.argmax(y_train_mnist[:200], 1), \n",
    "                 np.argmax(my_cnn.predict(X_train_mnist[:200].reshape(-1, 28,28,1)), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = my_cnn.layers[0].W\n",
    "plt.figure(figsize=(10, 5)) \n",
    "for i in range(10):\n",
    "    l1_plot = plt.subplot(2, 5, i + 1)\n",
    "    l1_plot.imshow(np.squeeze(weights[i]), interpolation='nearest',\n",
    "                   cmap=plt.cm.gray)\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(()) \n",
    "plt.suptitle(\"CNN first layers weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recurrent Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
