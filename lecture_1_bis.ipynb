{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Learning rate and  Training Algorithms\n",
    "The **learning rate** is usually chosen experimentally based on the figure bellow. Note that in practice variants of the gradient descent formula are used; some implementation for 3D visualization can be found [here](Gradient_Methods.ipynb) and of course in the [Keras library](https://keras.io/optimizers/):\n",
    "- Stochastic Gradient Descent with Momentum (**SGD+Momentum**)\n",
    "- [**Adadelta**](https://arxiv.org/abs/1212.5701)\n",
    "- [**Adam**](https://arxiv.org/pdf/1412.6980v8.pdf)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/learningrates.jpeg\" width=\"450\" title=\"Learning Rates\" >\n",
    "</p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Avoid over-fitting\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/fittings.jpg\" width=\"550\" title=\"Types of fittings during training.\" >\n",
    "</p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the data into three folds\n",
    "<p align=\"center\">\n",
    "<img src=\"images/training_splits.png\" width=\"500\" title=\"Dataset separation.\" >\n",
    "</p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Assessing the model performance on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/accuracies.jpeg\" width=\"350\" title=\"Dataset separation.\" >\n",
    "</p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Further readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Deep learning book](http://deeplearningbook.org/)\n",
    "- [Stanford's CS231n](http://cs231n.github.io/)\n",
    "- [Washington University in St. Louis](https://github.com/jeffheaton/t81_558_deep_learning)\n",
    "- [Deep learning paper](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)\n",
    "- [Tensorflow/Keras tutorial](https://www.tensorflow.org/guide/keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 MNIST (handwritten digit classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "import torch.nn as nn # contains methods to create neural networks\n",
    "import torchvision as tv # allows access to various computer vision databases and image processing functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define here the set of transformations to apply on each image before getting to the network\n",
    "1. We transform the vector to a Torch tensor \n",
    "2. We flatten the image: from $(1 \\times 28 \\times 28)$ to a vector of size $784$\n",
    "3. First we normalize the image : (image - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tv.transforms.Compose([tv.transforms.ToTensor(),\n",
    "                              tv.transforms.Normalize(0.5, 0.5),\n",
    "                              tv.transforms.Lambda(lambda x: tc.flatten(x))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is downloaded and store in the folder $\\textit{./mnist}$ (if not already downloaded).\n",
    "\n",
    "In the same process, the training images are transformed using the previously defined transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only : trainingset = tv.datasets.MNIST(root=\"./mnist\", train=True, download=True, transform=transform)\n",
    "# is supposed to be enough to download the dataset.\n",
    "# But you may run into an HTTP error with some of the ressources on Yann LeCun's page being unavailable\n",
    "\n",
    "# See the issue here: https://github.com/pytorch/vision/issues/3549\n",
    "\n",
    "# The following ressources line is a just a temporary fix\n",
    "\n",
    "tv.datasets.MNIST.resources = [\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz', 'f68b3c2dcbeaaa9fbdd348bbdeb94873'),\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz', 'd53e105ee54ea40749a09fcbcd1e9432'),\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz', '9fb629c4189551a2d022fa330f9573f3'),\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz', 'ec29112dd5afa0611ce80d1b7f02629c')\n",
    "        ]\n",
    "\n",
    "trainingset = tv.datasets.MNIST(root=\"./mnist\", train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a loader i.e a variable that can be used to (easily) navigate through our trainingset.\n",
    "1. Our loader will return a batch of 64 images along with their labels each time it is called\n",
    "2. Also, the loader shuffles the dataset before sampling the 64 images before each call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = tc.utils.data.DataLoader(trainingset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now define our Neural network. It's a simple multi-layer perceptron consisting of:\n",
    "1. a Dense layer\n",
    "2. a ReLU activation layer\n",
    "3. a Dropout layer\n",
    "4. and a Softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, n_classes,  dropout_prob=0.3):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(n_inputs, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_classes),\n",
    "            #nn.Dropout(p=dropout_prob),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our images are vectors of size 784\n",
    "# and we have 10 classes\n",
    "\n",
    "net = MLP(n_inputs=784, n_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only things left are \n",
    "- the loss function\n",
    "- and the optimizer\n",
    "\n",
    "We will be using the Cross-Entropy loss function.\n",
    "And for the optimizer a standard Stochastic Gradient Descent (SGD) should be good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function definition\n",
    "# CrossEntropy since we have a Softmax as last layer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers takes as input parameters to optimize and a learning rate\n",
    "optimizer = tc.optim.SGD(net.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let the training begin now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 # overall number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - loss 2.301060438156128\n",
      "Epoch 2/100 - loss 2.3007211685180664\n",
      "Epoch 3/100 - loss 2.3013782501220703\n",
      "Epoch 4/100 - loss 2.2947072982788086\n",
      "Epoch 5/100 - loss 2.2953991889953613\n",
      "Epoch 6/100 - loss 2.3019230365753174\n",
      "Epoch 7/100 - loss 2.29240083694458\n",
      "Epoch 8/100 - loss 2.296236753463745\n",
      "Epoch 9/100 - loss 2.294034481048584\n",
      "Epoch 10/100 - loss 2.2830793857574463\n",
      "Epoch 11/100 - loss 2.2933497428894043\n",
      "Epoch 12/100 - loss 2.290843963623047\n",
      "Epoch 13/100 - loss 2.2861928939819336\n",
      "Epoch 14/100 - loss 2.2870659828186035\n",
      "Epoch 15/100 - loss 2.2778518199920654\n",
      "Epoch 16/100 - loss 2.270634889602661\n",
      "Epoch 17/100 - loss 2.2710959911346436\n",
      "Epoch 18/100 - loss 2.2508740425109863\n",
      "Epoch 19/100 - loss 2.2316088676452637\n",
      "Epoch 20/100 - loss 2.2321789264678955\n",
      "Epoch 21/100 - loss 2.2499282360076904\n",
      "Epoch 22/100 - loss 2.2377519607543945\n",
      "Epoch 23/100 - loss 2.2352070808410645\n",
      "Epoch 24/100 - loss 2.208195209503174\n",
      "Epoch 25/100 - loss 2.336005687713623\n",
      "Epoch 26/100 - loss 2.220587730407715\n",
      "Epoch 27/100 - loss 2.2398269176483154\n",
      "Epoch 28/100 - loss 2.245237112045288\n",
      "Epoch 29/100 - loss 2.2477707862854004\n",
      "Epoch 30/100 - loss 2.2151565551757812\n",
      "Epoch 31/100 - loss 2.1788015365600586\n",
      "Epoch 32/100 - loss 2.248584032058716\n",
      "Epoch 33/100 - loss 2.191197156906128\n",
      "Epoch 34/100 - loss 2.231551170349121\n",
      "Epoch 35/100 - loss 2.216207504272461\n",
      "Epoch 36/100 - loss 2.1827890872955322\n",
      "Epoch 37/100 - loss 2.1592800617218018\n",
      "Epoch 38/100 - loss 2.1076250076293945\n",
      "Epoch 39/100 - loss 2.111475944519043\n",
      "Epoch 40/100 - loss 2.158501148223877\n",
      "Epoch 41/100 - loss 2.1658616065979004\n",
      "Epoch 42/100 - loss 2.1061487197875977\n",
      "Epoch 43/100 - loss 2.10866641998291\n",
      "Epoch 44/100 - loss 2.1353964805603027\n",
      "Epoch 45/100 - loss 2.172213554382324\n",
      "Epoch 46/100 - loss 2.1418416500091553\n",
      "Epoch 47/100 - loss 2.077265977859497\n",
      "Epoch 48/100 - loss 2.1628096103668213\n",
      "Epoch 49/100 - loss 2.0748043060302734\n",
      "Epoch 50/100 - loss 2.0244193077087402\n",
      "Epoch 51/100 - loss 2.124706745147705\n",
      "Epoch 52/100 - loss 2.0889294147491455\n",
      "Epoch 53/100 - loss 2.026250123977661\n",
      "Epoch 54/100 - loss 2.0612094402313232\n",
      "Epoch 55/100 - loss 2.1540558338165283\n",
      "Epoch 56/100 - loss 2.0479490756988525\n",
      "Epoch 57/100 - loss 2.001725673675537\n",
      "Epoch 58/100 - loss 1.9534027576446533\n",
      "Epoch 59/100 - loss 1.9742358922958374\n",
      "Epoch 60/100 - loss 2.142188549041748\n",
      "Epoch 61/100 - loss 2.0247154235839844\n",
      "Epoch 62/100 - loss 1.975325345993042\n",
      "Epoch 63/100 - loss 2.000239610671997\n",
      "Epoch 64/100 - loss 2.1499133110046387\n",
      "Epoch 65/100 - loss 1.984521508216858\n",
      "Epoch 66/100 - loss 1.9060462713241577\n",
      "Epoch 67/100 - loss 2.0310330390930176\n",
      "Epoch 68/100 - loss 1.9390554428100586\n",
      "Epoch 69/100 - loss 1.9301445484161377\n",
      "Epoch 70/100 - loss 1.9888417720794678\n",
      "Epoch 71/100 - loss 2.058380126953125\n",
      "Epoch 72/100 - loss 2.0265262126922607\n",
      "Epoch 73/100 - loss 2.08967661857605\n",
      "Epoch 74/100 - loss 2.068814992904663\n",
      "Epoch 75/100 - loss 1.9398601055145264\n",
      "Epoch 76/100 - loss 1.9536710977554321\n",
      "Epoch 77/100 - loss 1.8546192646026611\n",
      "Epoch 78/100 - loss 1.9799662828445435\n",
      "Epoch 79/100 - loss 1.8546607494354248\n",
      "Epoch 80/100 - loss 1.8608256578445435\n",
      "Epoch 81/100 - loss 1.7698051929473877\n",
      "Epoch 82/100 - loss 1.8846615552902222\n",
      "Epoch 83/100 - loss 1.7582635879516602\n",
      "Epoch 84/100 - loss 1.8759980201721191\n",
      "Epoch 85/100 - loss 1.94172203540802\n",
      "Epoch 86/100 - loss 1.8349229097366333\n",
      "Epoch 87/100 - loss 1.7949732542037964\n",
      "Epoch 88/100 - loss 1.8290190696716309\n",
      "Epoch 89/100 - loss 1.9219876527786255\n",
      "Epoch 90/100 - loss 1.872076153755188\n",
      "Epoch 91/100 - loss 1.8204320669174194\n",
      "Epoch 92/100 - loss 1.8507654666900635\n",
      "Epoch 93/100 - loss 1.7625609636306763\n",
      "Epoch 94/100 - loss 1.9925709962844849\n",
      "Epoch 95/100 - loss 1.8002612590789795\n",
      "Epoch 96/100 - loss 1.9148844480514526\n",
      "Epoch 97/100 - loss 1.8892736434936523\n",
      "Epoch 98/100 - loss 1.7031275033950806\n",
      "Epoch 99/100 - loss 1.7622005939483643\n",
      "Epoch 100/100 - loss 1.7964916229248047\n"
     ]
    }
   ],
   "source": [
    "iter_train = iter(trainloader) # converting the loader to a Python iterator, so we can use 'next' to navigate \n",
    "for e in range(epochs):\n",
    "    images, labels = next(iter_train) # getting the images and their labels\n",
    "\n",
    "    optimizer.zero_grad() # Zero-out the gradients, cleaning the optimizer\n",
    "\n",
    "    output = net.forward(images) # we apply the model on the batch images\n",
    "\n",
    "    loss = criterion(output, labels) # we compute the loss between the model's outputs and the labels\n",
    "\n",
    "    loss.backward() # we backpropagate the loss in the model to compute all the gradiants\n",
    "\n",
    "    optimizer.step() # final we update the weights\n",
    "    \n",
    "    print(\"Epoch {}/{} - loss {}\".format(e+1, epochs, loss.detach().numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingset = tv.datasets.MNIST(root=\"./mnist\", train=False, download=True, transform=transform)\n",
    "testloader = tc.utils.data.DataLoader(testingset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) tensor([2])\n",
      "tensor(1) tensor([5])\n",
      "tensor(1) tensor([6])\n",
      "tensor(9) tensor([9])\n",
      "tensor(9) tensor([8])\n",
      "tensor(1) tensor([4])\n",
      "tensor(1) tensor([2])\n",
      "tensor(1) tensor([3])\n",
      "tensor(1) tensor([9])\n",
      "tensor(1) tensor([9])\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    images, labels = next(iter(testloader))\n",
    "    with tc.no_grad():\n",
    "        output = net.forward(images)  \n",
    "\n",
    "    print(tc.argmax(output), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
