{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4b8412",
   "metadata": {},
   "source": [
    "# Fully Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b029c3",
   "metadata": {},
   "source": [
    "Example of a fully convolutional network from the [paper](http://cvlab.postech.ac.kr/research/deconvnet/)\n",
    "![Fully convolution net](../images/fcn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a861d",
   "metadata": {},
   "source": [
    "## 2.1. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61b5f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tramx/Library/Python/3.8/lib/python/site-packages/IPython/core/magics/pylab.py:162: UserWarning: pylab import has clobbered these variables: ['pylab']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pylab\n",
    "%pylab inline\n",
    "\n",
    "from skimage import io, transform, exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1da381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b1e3a",
   "metadata": {},
   "source": [
    "### Building a Simple Segmentation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e49a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSegNet(nn.Module):\n",
    "    def __init__(self, n_inputs, n_classes=1,  dropout_prob=0.3):\n",
    "        super(SimpleSegNet, self).__init__()\n",
    "\n",
    "        self.pipe = nn.Sequential(\n",
    "            # Encoder\n",
    "            nn.Conv2d(n_inputs, 8, (3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, (3,3), stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 16, (3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (3,3), stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, (3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Decoder\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(32, 16, (3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(16, 8, (3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 1, (3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Softmax(dim=-1) if n_classes > 1 else nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8607d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_seg_net = SimpleSegNet(n_inputs=3, n_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90fe3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 128, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_seg_net.forward(tc.randn(12, 3, 128, 128)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96396f",
   "metadata": {},
   "source": [
    "### Loading a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d6d7cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplab = tv.models.segmentation.deeplabv3_resnet50(pretrained=False, num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36590b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 256, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplab.forward(tc.randn(12, 3, 256, 256))[\"out\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d8b05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function definition\n",
    "# CrossEntropy since we have a Softmax as last layer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74020d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67adaaaf",
   "metadata": {},
   "source": [
    "# VOC Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a66d04",
   "metadata": {},
   "source": [
    "# Retina Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3c29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"./DRIVE/\"\n",
    "task = \"training\" # \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bbdd744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "518ed455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriveDataset(tc.utils.data.Dataset):\n",
    "    \"\"\"Blood vessel segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, task, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task (string): current task, either \"training\" or \"test\".\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self._build_list_images(task)\n",
    "    \n",
    "    def _build_list_images(self, task):\n",
    "        self.im_paths = []\n",
    "        self.gt_paths = []\n",
    "        for i in range(21, 41):\n",
    "            self.im_paths.append(os.path.join(task, \"images\", \"{0}_training.tif\".format(i)))\n",
    "            self.gt_paths.append(os.path.join(task, \"1st_manual\", \"{0}_manual1.gif\".format(i)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if tc.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        im_name = os.path.join(self.root_dir, self.im_paths[idx])\n",
    "        gt_name = os.path.join(self.root_dir, self.gt_paths[idx])\n",
    "        image = io.imread(im_name)\n",
    "        label = io.imread(gt_name)\n",
    "        print(np.unique(image), np.unique(label))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a28fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62804ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tv.transforms.Compose([tv.transforms.ToTensor(),\n",
    "                              #tv.transforms.Normalize(0.5, 0.5),\n",
    "                              tv.transforms.RandomCrop((256, 256))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2eb313c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retina_dataset = DriveDataset(task, ROOT_FOLDER, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "deab9155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255] [  0 255]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.8745, 0.8667, 0.8510],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.8510, 0.8510, 0.8275],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.8392, 0.8392, 0.8196],\n",
       "          ...,\n",
       "          [0.1961, 0.2784, 0.4549,  ..., 0.7686, 0.7843, 0.7922],\n",
       "          [0.1137, 0.1569, 0.2510,  ..., 0.7647, 0.7765, 0.7882],\n",
       "          [0.0275, 0.0824, 0.1725,  ..., 0.7804, 0.7882, 0.7922]],\n",
       " \n",
       "         [[0.9569, 0.9373, 0.9412,  ..., 0.4549, 0.4510, 0.4353],\n",
       "          [0.9569, 0.9608, 0.9686,  ..., 0.4353, 0.4510, 0.4275],\n",
       "          [0.9490, 0.9569, 0.9608,  ..., 0.4196, 0.4392, 0.4196],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0588,  ..., 0.4235, 0.4353, 0.4431],\n",
       "          [0.0118, 0.0000, 0.0000,  ..., 0.4118, 0.4118, 0.4235],\n",
       "          [0.0353, 0.0078, 0.0000,  ..., 0.4196, 0.4275, 0.4314]],\n",
       " \n",
       "         [[0.5294, 0.5098, 0.5098,  ..., 0.2902, 0.2941, 0.2824],\n",
       "          [0.5255, 0.5333, 0.5373,  ..., 0.2824, 0.2941, 0.2784],\n",
       "          [0.5176, 0.5255, 0.5294,  ..., 0.2863, 0.2902, 0.2706],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0902,  ..., 0.2745, 0.2784, 0.2863],\n",
       "          [0.0235, 0.0000, 0.0000,  ..., 0.2745, 0.2824, 0.2941],\n",
       "          [0.0314, 0.0314, 0.0000,  ..., 0.2745, 0.2824, 0.2863]]]),\n",
       " tensor([[[0., 0., 0.,  ..., 1., 1., 1.],\n",
       "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retina_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1902f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = tc.utils.data.DataLoader(retina_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8a0d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_seg_net = SimpleSegNet(n_inputs=3, n_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6158a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function definition\n",
    "# CrossEntropy since we have a Softmax as last layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizers takes as input parameters to optimize and a learning rate\n",
    "optimizer = tc.optim.SGD(simple_seg_net.parameters(), lr=0.5)\n",
    "epochs = 100 # overall number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b7003b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - loss -0.0\n",
      "Epoch 1/100 - loss -0.0\n",
      "Epoch 1/100 - loss -0.0\n",
      "Epoch 1/100 - loss -0.0\n",
      "Epoch 1/100 - loss -0.0\n",
      "Epoch 2/100 - loss -0.0\n",
      "Epoch 2/100 - loss -0.0\n",
      "Epoch 2/100 - loss -0.0\n",
      "Epoch 2/100 - loss -0.0\n",
      "Epoch 2/100 - loss -0.0\n",
      "Epoch 3/100 - loss -0.0\n",
      "Epoch 3/100 - loss -0.0\n",
      "Epoch 3/100 - loss -0.0\n",
      "Epoch 3/100 - loss -0.0\n",
      "Epoch 3/100 - loss -0.0\n",
      "Epoch 4/100 - loss -0.0\n",
      "Epoch 4/100 - loss -0.0\n",
      "Epoch 4/100 - loss -0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m sample_batched \u001b[38;5;66;03m# getting the images and their labels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zero-out the gradients, cleaning the optimizer\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msimple_seg_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# we apply the model on the batch images\u001b[39;00m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels) \u001b[38;5;66;03m# we compute the loss between the model's outputs and the labels\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# we backpropagate the loss in the model to compute all the gradiants\u001b[39;00m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mSimpleSegNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "losses = []\n",
    "for e in range(epochs):\n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        images, labels = sample_batched # getting the images and their labels\n",
    "\n",
    "        optimizer.zero_grad() # Zero-out the gradients, cleaning the optimizer\n",
    "\n",
    "        output = simple_seg_net.forward(images) # we apply the model on the batch images\n",
    "\n",
    "        loss = criterion(output, labels) # we compute the loss between the model's outputs and the labels\n",
    "\n",
    "        loss.backward() # we backpropagate the loss in the model to compute all the gradiants\n",
    "\n",
    "        optimizer.step() # final we update the weights\n",
    "\n",
    "        print(\"Epoch {}/{} - loss {}\".format(e+1, epochs, loss.detach().numpy()))\n",
    "        losses.append(loss.detach().numpy())\n",
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baadf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
